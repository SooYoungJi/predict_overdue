{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e765b9a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T00:23:33.023404Z",
     "start_time": "2022-05-10T00:22:51.546721Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from vecstack import stacking, StackingTransformer\n",
    "\n",
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "\n",
    "from tensorflow.keras import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6bff32",
   "metadata": {},
   "source": [
    "# pycaret으로 직업 예측 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5ac813",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "469d4300",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T00:23:33.722502Z",
     "start_time": "2022-05-10T00:23:33.044282Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train_occpy_pred_final.csv')\n",
    "test = pd.read_csv('./data/test_occpy_pred_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5de98a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T00:23:34.209733Z",
     "start_time": "2022-05-10T00:23:33.926880Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>car</th>\n",
       "      <th>reality</th>\n",
       "      <th>child_num</th>\n",
       "      <th>income_total</th>\n",
       "      <th>income_type</th>\n",
       "      <th>edu_type</th>\n",
       "      <th>family_type</th>\n",
       "      <th>house_type</th>\n",
       "      <th>DAYS_EMPLOYED</th>\n",
       "      <th>work_phone</th>\n",
       "      <th>phone</th>\n",
       "      <th>email</th>\n",
       "      <th>occyp_type</th>\n",
       "      <th>family_size</th>\n",
       "      <th>begin_month</th>\n",
       "      <th>credit</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>Commercial associate</td>\n",
       "      <td>Higher education</td>\n",
       "      <td>Married</td>\n",
       "      <td>Municipal apartment</td>\n",
       "      <td>4709</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Accountants</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>247500.0</td>\n",
       "      <td>Commercial associate</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Civil marriage</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>1540</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Laborers</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>450000.0</td>\n",
       "      <td>Working</td>\n",
       "      <td>Higher education</td>\n",
       "      <td>Married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>4434</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Managers</td>\n",
       "      <td>2.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender  car  reality  child_num  income_total           income_type  \\\n",
       "0       0    0        0          0      202500.0  Commercial associate   \n",
       "1       0    0        1          1      247500.0  Commercial associate   \n",
       "2       1    1        1          0      450000.0               Working   \n",
       "\n",
       "                        edu_type     family_type           house_type  \\\n",
       "0               Higher education         Married  Municipal apartment   \n",
       "1  Secondary / secondary special  Civil marriage    House / apartment   \n",
       "2               Higher education         Married    House / apartment   \n",
       "\n",
       "   DAYS_EMPLOYED  work_phone  phone  email   occyp_type  family_size  \\\n",
       "0           4709           0      0      0  Accountants          2.0   \n",
       "1           1540           0      0      1     Laborers          3.0   \n",
       "2           4434           0      1      0     Managers          2.0   \n",
       "\n",
       "   begin_month  credit  age  \n",
       "0          6.0     1.0   38  \n",
       "1          5.0     1.0   31  \n",
       "2         22.0     2.0   52  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.gender = train.gender.replace({'F' : 0, 'M' : 1})\n",
    "train.car = train.car.replace({'N' : 0, 'Y' : 1})\n",
    "train.reality = train.reality.replace({'N' : 0, 'Y' : 1})\n",
    "train['age'] = train.DAYS_BIRTH.apply(lambda x : -x // 365)\n",
    "train.DAYS_EMPLOYED = (-1) * train.DAYS_EMPLOYED \n",
    "train.loc[(train.DAYS_EMPLOYED < 0), 'DAYS_EMPLOYED'] = 0\n",
    "train.begin_month = (-1) * train.begin_month\n",
    "\n",
    "train = train.drop(['Unnamed: 0','DAYS_BIRTH'], axis = 1)\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c63fe6a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T00:23:34.577372Z",
     "start_time": "2022-05-10T00:23:34.371302Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>car</th>\n",
       "      <th>reality</th>\n",
       "      <th>child_num</th>\n",
       "      <th>income_total</th>\n",
       "      <th>income_type</th>\n",
       "      <th>edu_type</th>\n",
       "      <th>family_type</th>\n",
       "      <th>house_type</th>\n",
       "      <th>DAYS_EMPLOYED</th>\n",
       "      <th>work_phone</th>\n",
       "      <th>phone</th>\n",
       "      <th>email</th>\n",
       "      <th>occyp_type</th>\n",
       "      <th>family_size</th>\n",
       "      <th>begin_month</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112500.0</td>\n",
       "      <td>Pensioner</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Civil marriage</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Security staff</td>\n",
       "      <td>2.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>State servant</td>\n",
       "      <td>Higher education</td>\n",
       "      <td>Married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>8671</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Core staff</td>\n",
       "      <td>2.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>69372.0</td>\n",
       "      <td>Working</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>217</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Laborers</td>\n",
       "      <td>2.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender  car  reality  child_num  income_total    income_type  \\\n",
       "0       1    1        0          0      112500.0      Pensioner   \n",
       "1       0    0        1          0      135000.0  State servant   \n",
       "2       0    0        1          0       69372.0        Working   \n",
       "\n",
       "                        edu_type     family_type         house_type  \\\n",
       "0  Secondary / secondary special  Civil marriage  House / apartment   \n",
       "1               Higher education         Married  House / apartment   \n",
       "2  Secondary / secondary special         Married  House / apartment   \n",
       "\n",
       "   DAYS_EMPLOYED  work_phone  phone  email      occyp_type  family_size  \\\n",
       "0              0           0      1      0  Security staff          2.0   \n",
       "1           8671           0      1      0      Core staff          2.0   \n",
       "2            217           1      1      0        Laborers          2.0   \n",
       "\n",
       "   begin_month  age  \n",
       "0         60.0   60  \n",
       "1         36.0   51  \n",
       "2         40.0   43  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.gender = test.gender.replace({'F' : 0, 'M' : 1})\n",
    "test.car = test.car.replace({'N' : 0, 'Y' : 1})\n",
    "test.reality = test.reality.replace({'N' : 0, 'Y' : 1})\n",
    "test['age'] = test.DAYS_BIRTH.apply(lambda x : -x // 365)\n",
    "test.DAYS_EMPLOYED = (-1) * test.DAYS_EMPLOYED \n",
    "test.loc[(test.DAYS_EMPLOYED < 0), 'DAYS_EMPLOYED'] = 0\n",
    "test.begin_month = (-1) * test.begin_month\n",
    "\n",
    "test = test.drop(['Unnamed: 0','DAYS_BIRTH'], axis = 1)\n",
    "test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c80b60f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T00:23:40.059826Z",
     "start_time": "2022-05-10T00:23:40.042214Z"
    }
   },
   "outputs": [],
   "source": [
    "y = train.credit\n",
    "X = train.drop(['credit'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72803a94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T06:03:56.278256Z",
     "start_time": "2022-05-08T06:03:56.264254Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd3f8e2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T06:03:56.352967Z",
     "start_time": "2022-05-08T06:03:56.340050Z"
    }
   },
   "outputs": [],
   "source": [
    "numeric_features = ['child_num', 'income_total', 'DAYS_EMPLOYED', 'family_size', 'begin_month', 'age']\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "categorical_features = ['income_type', 'edu_type', 'family_type', 'house_type','occyp_type']\n",
    "categorical_transformer = OneHotEncoder(categories='auto', handle_unknown = 'ignore')\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "                transformers=[\n",
    "                    ('num', numeric_transformer, numeric_features),\n",
    "                    ('cat', categorical_transformer, categorical_features)\n",
    "                ], remainder='passthrough'\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2297e2b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T06:03:56.503604Z",
     "start_time": "2022-05-08T06:03:56.415736Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocessor.fit(X_train)\n",
    "scaled_X_train = preprocessor.transform(X_train)\n",
    "scaled_X_test = preprocessor.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3657a5",
   "metadata": {},
   "source": [
    "## scaled_X_train, y_train 으로 학습\n",
    "## scaled_X_test, y_test 로 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1070a4",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6f97db7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-06T13:26:32.366717Z",
     "start_time": "2022-05-06T13:26:32.346572Z"
    }
   },
   "outputs": [],
   "source": [
    "xgb_clf = XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
    "               colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,\n",
    "               early_stopping_rounds=None, enable_categorical=False,\n",
    "               eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
    "               importance_type=None, interaction_constraints='',\n",
    "               learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,\n",
    "               max_delta_step=0, max_depth=8, max_leaves=0, min_child_weight=2,\n",
    "               missing=np.nan, monotone_constraints='()', n_estimators=170,\n",
    "               n_jobs=-1, num_parallel_tree=1, objective='multi:softprob',\n",
    "               predictor='auto', random_state=42, reg_alpha=10)\n",
    "\n",
    "lgbm_clf = LGBMClassifier(bagging_fraction=0.8, bagging_freq=3, boosting_type='gbdt',\n",
    "                class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,\n",
    "                importance_type='split', learning_rate=0.2, max_depth=-1,\n",
    "                min_child_samples=6, min_child_weight=0.001, min_split_gain=0.6,\n",
    "                n_estimators=100, n_jobs=-1, num_leaves=30, objective=None,\n",
    "                random_state=42, reg_alpha=0.001, reg_lambda=5, silent='warn',\n",
    "                subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
    "                            learning_rate=0.1, loss='deviance', max_depth=8,\n",
    "                            max_features='sqrt', max_leaf_nodes=None,\n",
    "                            min_impurity_decrease=0.001, min_impurity_split=None,\n",
    "                            min_samples_leaf=5, min_samples_split=10,\n",
    "                            min_weight_fraction_leaf=0.0, n_estimators=290,\n",
    "                            n_iter_no_change=None, \n",
    "                            random_state=42, subsample=0.25, tol=0.0001,\n",
    "                            validation_fraction=0.1, verbose=0,\n",
    "                            warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1234188",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-06T13:30:54.787528Z",
     "start_time": "2022-05-06T13:29:49.040370Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:         [classification]\n",
      "n_classes:    [3]\n",
      "metric:       [accuracy_score]\n",
      "mode:         [oof_pred_bag]\n",
      "n_models:     [3]\n",
      "\n",
      "model  0:     [XGBClassifier]\n",
      "[22:29:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"max_cat_to_onehot\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:29:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:29:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"max_cat_to_onehot\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:29:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:29:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"max_cat_to_onehot\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:29:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:29:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"max_cat_to_onehot\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:29:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:30:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"max_cat_to_onehot\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:30:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "    ----\n",
      "    MEAN:     [0.69969289] + [0.00308887]\n",
      "    FULL:     [0.69969289]\n",
      "\n",
      "model  1:     [LGBMClassifier]\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "    ----\n",
      "    MEAN:     [0.69562958] + [0.00344488]\n",
      "    FULL:     [0.69562958]\n",
      "\n",
      "model  2:     [GradientBoostingClassifier]\n",
      "    ----\n",
      "    MEAN:     [0.69624380] + [0.00369440]\n",
      "    FULL:     [0.69624380]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = [xgb_clf, lgbm_clf, gb_clf]\n",
    "\n",
    "S_train, S_test = stacking(models, \n",
    "                       scaled_X_train, y_train, scaled_X_test, \n",
    "                       regression = False, \n",
    "                       metric = accuracy_score,\n",
    "#                        needs_proba = True,                           \n",
    "                       n_folds = 5, stratified = True, shuffle = True, \n",
    "                       random_state = 0, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a76a8e6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-06T13:31:54.813954Z",
     "start_time": "2022-05-06T13:31:54.803980Z"
    }
   },
   "outputs": [],
   "source": [
    "xgb_final = XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
    "               colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,\n",
    "               early_stopping_rounds=None, enable_categorical=False,\n",
    "               eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
    "               importance_type=None, interaction_constraints='',\n",
    "               learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,\n",
    "               max_delta_step=0, max_depth=8, max_leaves=0, min_child_weight=2,\n",
    "               missing=np.nan, monotone_constraints='()', n_estimators=170,\n",
    "               n_jobs=-1, num_parallel_tree=1, objective='multi:softprob',\n",
    "               predictor='auto', random_state=42, reg_alpha=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4629afc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-06T13:32:58.628072Z",
     "start_time": "2022-05-06T13:32:57.729392Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:32:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"max_cat_to_onehot\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:32:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xgb_final.fit(S_train, y_train)\n",
    "pred_proba = xgb_final.predict_proba(S_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c428461",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-06T13:36:24.571211Z",
     "start_time": "2022-05-06T13:36:24.545280Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7106953892668179\n"
     ]
    }
   ],
   "source": [
    "pred = xgb_final.predict(S_test)\n",
    "print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950b8b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(y_test, pred_proba)\n",
    "# 0.7652126036905378"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4d1273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf54f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7184c0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T06:13:17.082339Z",
     "start_time": "2022-05-08T06:13:17.066382Z"
    }
   },
   "outputs": [],
   "source": [
    "train_y = train.credit\n",
    "train_X = train.drop('credit', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "163b417a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T06:13:17.448765Z",
     "start_time": "2022-05-08T06:13:17.440748Z"
    }
   },
   "outputs": [],
   "source": [
    "numeric_features = ['child_num', 'income_total', 'DAYS_EMPLOYED', 'family_size', 'begin_month', 'age']\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "categorical_features = ['income_type', 'edu_type', 'family_type', 'house_type','occyp_type']\n",
    "categorical_transformer = OneHotEncoder(categories='auto', handle_unknown = 'ignore')\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "                transformers=[\n",
    "                    ('num', numeric_transformer, numeric_features),\n",
    "                    ('cat', categorical_transformer, categorical_features)\n",
    "                ], remainder='passthrough'\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7fef57e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T06:13:21.362745Z",
     "start_time": "2022-05-08T06:13:21.244281Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocessor.fit(train_X)\n",
    "scaled_train = preprocessor.transform(train_X)\n",
    "scaled_test = preprocessor.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4da7cfcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T06:13:23.360227Z",
     "start_time": "2022-05-08T06:13:23.344234Z"
    }
   },
   "outputs": [],
   "source": [
    "xgb_clf = XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
    "               colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,\n",
    "               early_stopping_rounds=None, enable_categorical=False,\n",
    "               eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
    "               importance_type=None, interaction_constraints='',\n",
    "               learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,\n",
    "               max_delta_step=0, max_depth=8, max_leaves=0, min_child_weight=2,\n",
    "               missing=np.nan, monotone_constraints='()', n_estimators=170,\n",
    "               n_jobs=-1, num_parallel_tree=1, objective='multi:softprob',\n",
    "               predictor='auto', random_state=42, reg_alpha=10)\n",
    "\n",
    "lgbm_clf = LGBMClassifier(bagging_fraction=0.8, bagging_freq=3, boosting_type='gbdt',\n",
    "                class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,\n",
    "                importance_type='split', learning_rate=0.2, max_depth=-1,\n",
    "                min_child_samples=6, min_child_weight=0.001, min_split_gain=0.6,\n",
    "                n_estimators=100, n_jobs=-1, num_leaves=30, objective=None,\n",
    "                random_state=42, reg_alpha=0.001, reg_lambda=5, silent='warn',\n",
    "                subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
    "                            learning_rate=0.1, loss='deviance', max_depth=8,\n",
    "                            max_features='sqrt', max_leaf_nodes=None,\n",
    "                            min_impurity_decrease=0.001, min_impurity_split=None,\n",
    "                            min_samples_leaf=5, min_samples_split=10,\n",
    "                            min_weight_fraction_leaf=0.0, n_estimators=290,\n",
    "                            n_iter_no_change=None, \n",
    "                            random_state=42, subsample=0.25, tol=0.0001,\n",
    "                            validation_fraction=0.1, verbose=0,\n",
    "                            warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cd6fb1da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T06:29:51.075838Z",
     "start_time": "2022-05-08T06:29:51.065865Z"
    }
   },
   "outputs": [],
   "source": [
    "estimators = [\n",
    "    ('XGB', xgb_clf),\n",
    "    ('LGBM', lgbm_clf),\n",
    "    ('GB', gb_clf)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fa81251b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T06:29:51.652533Z",
     "start_time": "2022-05-08T06:29:51.637090Z"
    }
   },
   "outputs": [],
   "source": [
    "stack = StackingTransformer(estimators,\n",
    "                       regression = False, \n",
    "                       metric = accuracy_score,                    \n",
    "                       n_folds = 5, stratified = True, shuffle = True, \n",
    "                       random_state = 0, verbose = 1)\n",
    "\n",
    "stack.fit(scaled_train, train_y)\n",
    "\n",
    "S_train = stack.transform(scaled_train)\n",
    "S_test = stack.transform(scaled_test)\n",
    "\n",
    "model = XGBClassifier(seed = 0, n_jobs = -1, learning_rate = 0.1, n_estimators = 100, max_depth = 3, eval_metric='mlogloss') \n",
    "model = model.fit(S_train, train_y)\n",
    "\n",
    "y_pred_proba = model.predict_proba(S_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dc0b5909",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T06:31:41.404811Z",
     "start_time": "2022-05-08T06:29:53.109340Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:         [classification]\n",
      "n_classes:    [3]\n",
      "metric:       [accuracy_score]\n",
      "variant:      [A]\n",
      "n_estimators: [3]\n",
      "\n",
      "estimator  0: [XGB: XGBClassifier]\n",
      "[15:29:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"max_cat_to_onehot\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:29:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:29:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"max_cat_to_onehot\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:29:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:30:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"max_cat_to_onehot\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:30:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:30:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"max_cat_to_onehot\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:30:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:30:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"max_cat_to_onehot\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:30:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "    MEAN:     [0.70586246] + [0.00285516]\n",
      "\n",
      "estimator  1: [LGBM: LGBMClassifier]\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "    MEAN:     [0.69773614] + [0.00339640]\n",
      "\n",
      "estimator  2: [GB: GradientBoostingClassifier]\n",
      "    MEAN:     [0.70578680] + [0.00477084]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StackingTransformer(estimators=[('XGB',\n",
       "                                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                               callbacks=None,\n",
       "                                               colsample_bylevel=1,\n",
       "                                               colsample_bynode=1,\n",
       "                                               colsample_bytree=0.7,\n",
       "                                               early_stopping_rounds=None,\n",
       "                                               enable_categorical=False,\n",
       "                                               eval_metric=None, gamma=0,\n",
       "                                               gpu_id=-1,\n",
       "                                               grow_policy='depthwise',\n",
       "                                               importance_type=None,\n",
       "                                               interaction_constraints='',\n",
       "                                               learning_rate=0.3, max_bin=256,\n",
       "                                               m...\n",
       "                                                num_leaves=30, random_state=42,\n",
       "                                                reg_alpha=0.001,\n",
       "                                                reg_lambda=5)),\n",
       "                                ('GB',\n",
       "                                 GradientBoostingClassifier(max_depth=8,\n",
       "                                                            max_features='sqrt',\n",
       "                                                            min_impurity_decrease=0.001,\n",
       "                                                            min_samples_leaf=5,\n",
       "                                                            min_samples_split=10,\n",
       "                                                            n_estimators=290,\n",
       "                                                            random_state=42,\n",
       "                                                            subsample=0.25))],\n",
       "                    metric=<function accuracy_score at 0x00000237BA2205E0>,\n",
       "                    n_folds=5, regression=False, shuffle=True, stratified=True,\n",
       "                    verbose=1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack.fit(scaled_train, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "24427ec3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T06:39:00.793904Z",
     "start_time": "2022-05-08T06:38:56.335754Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set was detected.\n",
      "Transforming...\n",
      "\n",
      "estimator  0: [XGB: XGBClassifier]\n",
      "    DONE\n",
      "\n",
      "estimator  1: [LGBM: LGBMClassifier]\n",
      "    DONE\n",
      "\n",
      "estimator  2: [GB: GradientBoostingClassifier]\n",
      "    DONE\n",
      "\n",
      "Transforming...\n",
      "\n",
      "estimator  0: [XGB: XGBClassifier]\n",
      "    DONE\n",
      "\n",
      "estimator  1: [LGBM: LGBMClassifier]\n",
      "    DONE\n",
      "\n",
      "estimator  2: [GB: GradientBoostingClassifier]\n",
      "    DONE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "S_train = stack.transform(scaled_train)\n",
    "S_test = stack.transform(scaled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f00dfc7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T06:39:28.893846Z",
     "start_time": "2022-05-08T06:39:28.072560Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "model = XGBClassifier(seed = 0, n_jobs = -1, learning_rate = 0.1, n_estimators = 100, max_depth = 3, eval_metric='mlogloss') \n",
    "model = model.fit(S_train, train_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6f19e299",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T06:39:41.630478Z",
     "start_time": "2022-05-08T06:39:41.608538Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. ... 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(S_test) \n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f9c7141c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T06:40:51.491899Z",
     "start_time": "2022-05-08T06:40:51.450002Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10300916 0.17039236 0.72659844]\n",
      " [0.10300916 0.17039236 0.72659844]\n",
      " [0.10300916 0.17039236 0.72659844]\n",
      " ...\n",
      " [0.10300916 0.17039236 0.72659844]\n",
      " [0.10300916 0.17039236 0.72659844]\n",
      " [0.10300916 0.17039236 0.72659844]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba = model.predict_proba(S_test) \n",
    "print(y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8e38062d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T06:49:08.902768Z",
     "start_time": "2022-05-08T06:49:08.875800Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-55-1d42f7cf5abb>:2: FutureWarning: Slicing a positional slice with .loc is not supported, and will raise TypeError in a future version.  Use .loc with labels or .iloc with positions instead.\n",
      "  submission.loc[:, 1:] = y_pred_proba\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26457</td>\n",
       "      <td>0.103009</td>\n",
       "      <td>0.170392</td>\n",
       "      <td>0.726598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26458</td>\n",
       "      <td>0.103009</td>\n",
       "      <td>0.170392</td>\n",
       "      <td>0.726598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26459</td>\n",
       "      <td>0.103009</td>\n",
       "      <td>0.170392</td>\n",
       "      <td>0.726598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26460</td>\n",
       "      <td>0.103009</td>\n",
       "      <td>0.170392</td>\n",
       "      <td>0.726598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26461</td>\n",
       "      <td>0.103009</td>\n",
       "      <td>0.170392</td>\n",
       "      <td>0.726598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>36452</td>\n",
       "      <td>0.103009</td>\n",
       "      <td>0.170392</td>\n",
       "      <td>0.726598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>36453</td>\n",
       "      <td>0.103009</td>\n",
       "      <td>0.170392</td>\n",
       "      <td>0.726598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>36454</td>\n",
       "      <td>0.103009</td>\n",
       "      <td>0.170392</td>\n",
       "      <td>0.726598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>36455</td>\n",
       "      <td>0.103009</td>\n",
       "      <td>0.170392</td>\n",
       "      <td>0.726598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>36456</td>\n",
       "      <td>0.103009</td>\n",
       "      <td>0.170392</td>\n",
       "      <td>0.726598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index         0         1         2\n",
       "0     26457  0.103009  0.170392  0.726598\n",
       "1     26458  0.103009  0.170392  0.726598\n",
       "2     26459  0.103009  0.170392  0.726598\n",
       "3     26460  0.103009  0.170392  0.726598\n",
       "4     26461  0.103009  0.170392  0.726598\n",
       "...     ...       ...       ...       ...\n",
       "9995  36452  0.103009  0.170392  0.726598\n",
       "9996  36453  0.103009  0.170392  0.726598\n",
       "9997  36454  0.103009  0.170392  0.726598\n",
       "9998  36455  0.103009  0.170392  0.726598\n",
       "9999  36456  0.103009  0.170392  0.726598\n",
       "\n",
       "[10000 rows x 4 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv('./data/sample_submission.csv')\n",
    "submission.loc[:, 1:] = y_pred_proba\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6d303c14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T06:49:21.902832Z",
     "start_time": "2022-05-08T06:49:21.827421Z"
    }
   },
   "outputs": [],
   "source": [
    "submission.to_csv('./data/submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aa1080",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514115f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b9cc48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9ef7ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7da970d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f101b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
